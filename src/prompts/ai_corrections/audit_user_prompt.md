---
version: v1
description: "Audit extraction output and propose feedback corrections"
requires:
  - vertical_name
  - items_json
---
You are auditing an entity extraction system that extracts brands and products from llm responses for the vertical "{{ vertical_name }}".

You will receive a JSON array of up to 5 items. Each item contains:
- prompt_zh / prompt_eng -> prompt to the llm in Chinese / English
- prompt_response_zh / prompt_response_en -> prompt response from the llm in Chinese / English, This is the text entity extraction system extracted brands/products from. From here you need to extract the ground truth!
- brands_extracted[] with brand_zh/brand_en -> extracted brands, text_snippet_zh/text_snippet_en -> text snippet from the prompt response the llm found the brand/product, rank -> position of the brand/product in the prompt (brands that occured first get rank=1), products_zh/products_en -> products extracted
The input JSON always looks like this:
{
  "vertical_name": <vertical name>
  "model": <llm model>
  "prompt_zh": <original, prompt in Chinese>,
  "prompt_eng": <translated prompt in english>,
  "prompt_response_zh": <original prompt response>,
  "prompt_response_en": <translated prompt response in english>,
  "brands_extracted": [
    {
      "brand_zh": <brand name extracted in chinese>,
      "brand_en": <brand name extracted in english>,
      "text_snippet_zh": <text snippet from which brand got extracted in chinese>,
      "text_snippet_en": <text snippet from which brand got extracted translated in en>,
      "rank": <int of the rank>,
      "products_zh": [<product of the brand extracted in original from the prompt>],
      "products_en": [<product of the brand extracted in original from the prompt>]
    }, ..
    # all brands from the prompt
  ], # all other prompts
}


Task:
1) Derive the ground-truth brands, products, and productâ†’brand mappings that are explicitly mentioned in prompt_response_zh.
2) Compare ground-truth vs brands_extracted and identify errors and missed items.
3) Output suggested feedback actions that would help the system improve for this vertical.

Rules:
- Only use information from the provided responses. Do not invent entities.
- Provide both confidence_level and confidence_score_0_1.
- confidence_level must be one of: LOW, MEDIUM, HIGH, VERY_HIGH.
- confidence_score_0_1 must be a number between 0 and 1.
- Evidence must be a short exact quote from prompt_response_zh.
- if the extraction system made a mistake provide a reason, e.g. "brand not relevant for the vertical" or "product has wrong brand" etc along with the Evidence

Return ONLY valid JSON with this shape:
{
  "items": [
    {
      "llm_answer_id": <int>,
      "truth": {
        "brands": [<string>],
        "products": [<string>],
        "mappings": [{"product": <string>, "brand": <string>}]
      },
      "suggestions": [
        {
          "category": <string>,
          "action": <string>,
          "brand_name": <string|null>,
          "product_name": <string|null>,
          "wrong_name": <string|null>,
          "correct_name": <string|null>,
          "reason": <string>,
          "evidence_quote_zh": <string>,
          "confidence_level": <string>,
          "confidence_score_0_1": <number>
        }
      ]
    }
  ]
}

Allowed actions:
- validate_brand, reject_brand, replace_brand
- validate_product, reject_product, replace_product
- add_mapping, validate_mapping, reject_mapping

Input JSON array:
{{ items_json }}

